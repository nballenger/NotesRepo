Foundations of Python Network Programming: The comprehensive guide to building network applications with Python, 2nd ed.

Chapter 1. Introduction to Client/Server Networking

* virtualenv is a very useful package for testing libraries

Chapter 2. UDP
    2.1 Should You Read This Chapter?
        * "If you even think you want to use the UDP protocol, then you probably want to use a message queue system instead."
        * "Use UDP only if you really want to be interacting with a very low level of the IP network stack."
    2.2 Addresses and Port Numbers
        * IPv4 addresses are four dot separated quads
        * port numbers follow a colon
    2.3 Port Number Ranges
        * Many port numbers are already spoken for--see /etc/services
        * DHCP is capable of some auto configuration on connection
        * There are three tiers of ports:
            # Well-Known, 0-1023, controlled by root user
            # Registered, 1024-49151, available to any user
            # Free pool, 49152-65535, used by the OS randomly
    2.4 Sockets
        * Sockets are sort of like file descriptors, for networking
        * You ask for a socket to be bound to a port
        * Basic socket usage example, server:
        
            import socket
            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            s.bind(('127.0.0.1', 1060))
            print 'Listening at', s.getsockname()
            while True:
                data, address = s.recvfrom(65535)
        
        * Opening a UDP socket happens via socket.SOCK_DGRAM
        * bind() requests a UDP network address, which is a combo of an IP address or hostname and a UDP port number
        * Basic socket usage example, client:
        
            import socket
            s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
            s.sendto('This is my message', ('127.0.0.1', PORT))
            data, address = s.recvfrom(65535)
            
    2.5 Unreliability, Backoff, Blocking, Timeouts
        * UDP is unreliable in that it doesn't automatically reply, or tell you about dropped packets.
        * Applications that use UDP should back off if they don't get responses, because they'll otherwise flood the network.
        * Imposing a rising delay between resends is a good backoff.
    2.6 Connecting UDP Sockets
        * a connect() call on a socket object obviates the need to use sendto() to send to a specific server/port
        * connect() also guarantees that the received data will come from the same server as was sent to (though is not secure against a dedicated man in the middle attack)
        * connect() does not send anything across the network by itself
    2.7 Request IDs: A Good Idea
        * It's a good idea to pass a request id with each UDP request sent, so that you can tell which request a response relates to.
        * Request IDs should be non-sequential as a very basic security measure
    2.8 Binding to Interfaces
        * You can bind a socket to a particular external IP of the machine your code is running on, and it will only listen for requests addressed to that IP
        * Requests originating on the localhost can send to that particular IP using any originating IP they have access to, 127.0.0.1 or otherwise.
        * Multiple processes cannot listen using the same IP address and port number
        * The IP network stack thinks of UDP in terms of "socket names" that are always a pair linking an IP interface and a UDP port number. Socket names are what cannot conflict among listening processes.
    2.9 UDP Fragmentation
        * IP will send small UDP packets singly, but will split up larger UDP packets into smaller physical packets.
        * Larger UDP packets are more likely to be dropped, since any dropped portion kills the entire packet.
        * You may want to find a way to detect the MTU (maximum transmission unit) between you and a target host, and regulate your packet size accordingly.
    2.10 Socket Options
        * Methods are getsockopt() and setsockopt()
        * Calls name the option group in which the option lives, then the option
        * Example calls:
            
            value = s.getsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST)
            s.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, value)
            
        * Common options: SO_BROADCAST, SO_DONTROUTE, SO_TYPE
        * See man socket(7), udp(7), tcp(7)
    2.11 Broadcast
        * Packets can be sent to an entire subnet at once
        * To use broadcast, turn it on with:
        
            s.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)
            
        * A subnet's broadcast address is xxx.xxx.xxx.255
    2.12 When to Use UDP
        * When you are using a protocol that already exists, and it uses UDP
        * "Because unreliable subnet broadcast is a great pattern for your application, and UDP supports it perfectly."
        
Chapter 3: TCP
    3.1 How TCP Works
        * TCP allows applications to send streams of data that, if they arrive, are guaranteed to arrive intact, in order, and without duplication.
        * TCP hides the packetization of data from your application, which only understands it as a stream.
        * Instead of sequential numbers, TCP uses a counter that counts the number of bytes transmitted. A 1024 byte packet with a sequence number of 7200 would be followed by a packet with a sequence number of 8224.
        * The initial sequence number is chosen randomly.
    3.2 When to Use TCP
        * When you want to send something larger than very small packets (since you incur an overhead in SYN-ACK/FIN-ACK communications)
        * When you want intelligent re-transmission of any lost packets
        * When you want flow control and exponential backoff built into the protocol
        * When you want a long-term relationship (multiple requests) between client and server
    3.3 What TCP Sockets Mean
        * Since TCP is stateful, connect() kicks off the handshake protocol and is fundamental
        * connect() can fail, since it does communicate over the network
        * On the server side, a new socket is created with a successful handshake
        * Two types of TCP sockets exist:
            # passive: holds the socket name (address and port), cannot send or receive, alerts the OS to willingness to receive incoming connections
            # active, connected: bound to one particular remote conversation partner, with their own IP address and port number. Can be used only for talking back and forth with that partner, and can be read and written to without worrying about how the resulting data will be packetized. In some cases can be passed around like a file handle.
        * Multiple active sockets can share the same socket name--the unique combination will be:
        
            (local_ip, local_port, remote_ip, remote_port)
            
    3.4 A Simple TCP Client and Server
        * Example server code:
        
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            
            def recv_all(sock, length):
                data = ''
                while len(data) < length:
                    more = sock.recv(length - len(data))
                    if not more:
                        raise EOFError
                    data += more
                return data
                
            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
            s.bind(('127.0.0.1', 1060))
            s.listen(1)
            while True:
                sc, sockname = s.accept()
                message = recv_all(sc, 16)
                sc.sendall('Farewell, client')
                sc.close()
            
        * Example client code:
        
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            s.connect(('127.0.0.1', 1060))
            s.sendall('Hi there, server')
            reply = recv_all(s,16)
            s.close()
            
        * A send() call can encounter three states:
            # Data can be immediately accepted by the system, because the network interface is free to transmit, or because the system has room to copy the data to a temp outgoing buffer. Full length of your data is immediately returned.
            # Network card is busy or outgoing buffer is full. send() blocks, pausing the program until the data can be accepted.
            # Outgoing buffers are _almost_ full, but not quite. Part of the data is accepted, but the rest has to wait. send() completes immediately and returns the number of bytes accepted from the beginning of your data string.
        * A send() call has to be wrapped like so:
        
            bytes_sent = 0
            while bytes_sent < len(message):
                message_remaining = message[bytes_sent:]
                bytes_sent += s.send(message_remaining)
                
        * The standard socket library gives us sendall() to bypass the need for that loop.
        * No equivalent exists for recv(), so you must loop it.
    3.5 One Socket per Conversation
        * Once a TCP socket has had listen() run on it, it cannot be used to send or receive
        * A listening socket can only receive incoming connections through accept()
        * The listening socket returns, via accept(), a new socket that is able to handle conversations
        * A socket returned by accept() works like a client socket, and can use sendall() and recv()
        * The integer argument to listen() tells it how many waiting connections can stack up (between being received and granted an active socket) before the OS turns new connections away with connection errors
    3.6 Address Already in Use
        * If you don't specify SO_REUSEADDR via setsockopt(), your socket may not be able to be reused for several minutes after it is terminated.
        * The OS will keep it open for a while, because it can't definitively tell if the client has received the final ACK packet issued
    3.7 Binding to Interfaces
        * Once you bind to an address and port, connections to any other port/address will be refused before reaching the application layer.
    3.8 Deadlock
        * If you attempt to send a very large stream of data, it is possible that both the server's output buffer and the client's input buffer have both filled up, and TCP has used its window adjustment protocol to signal this fact and stop the socket from sending more data that would have to be discarded and later resent.
        * Deadlocks can be avoided in two ways:
            # They can use socket options to turn off blocking, so that calls like send() and recv() return immediately if they find they cannot send any data yet.
            # They can process data from several inputs at a time, either by splitting into separate threads or processes--one tasked with sending data into a socket, perhaps, and another tasked with reading data back out--or by running OS calls like select() or poll() that let them wait on busy outgoing and incoming sockets at the same time, and respond to whichever is ready.
    3.9 Closed Connections, Half-Open Connections
        * There may be conditions in which it is appropriate for a socket to stop reading, or stop writing, without fully closing.
        * You can issue a shutdown() call to end either direction of communication in a two way socket:
            # SHUT_WR, indicates the caller will be writing no more data to the socket, and reads from the other end should act like it is closed.
            # SHUT_RD: Used to turn off the incoming socket stream, so an EOF error is encountered if your peer tries to send any more data to you on the socket.
            # SHUT_RDWR: Closes communication in both directions on the socket. Different from calling close() on the socket, because while close() merely ends your process's relationship with the socket, while shutdown() with SHUT_RDWR immediately disables the socket for all connected processes.
    3.10 Using TCP Streams like Files
        * Python makes sure that read() and write() are methods only called on file objects, and send() and recv() are only called on sockets--no object can do both.
        * You occasionally want to treat a socket like a file object, for passing to modules like pickle, json, zlib, etc.
        * To do that, you can use the socket's makefile() method:
        
            import socket
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            f = s.makefile()

Chapter 4: Socket Names and DNS            
    4.1 Hostnames and Domain Names
        * Terms:
            # TLD: set of possible suffixes for valid domain names
            # Domain name: suffix to an organization's sites and hosts on the internet
            # Fully qualified domain name: hostname + domain name + tld
            # Hostname: sometimes the machine name, sometimes synonym for FQDN
    4.2 Socket Names
        * Socketname is a tuple made of IP address or hostname and a port number
        * Socket methods that involve a socket name:
            # mysocket.accept(): called on a listening TCP stream socket, returns a tuple of the net socket connected to a remote address, and the remote address that has connected
            # mysocket.bind(address): assigns the socket the local address so that outgoing packets have an address from which to originate, and so incoming connections have a name they can use to connect
            # mysocket.connect(address): establishes that data sent through this socket will be directed to the given remote address.
            # mysocket.getpeername(): returns the remote address to which the socket is connected
            # mysocket.getsockname(): returns the address of this sockets local endpoint
            # mysocket.recvfrom(...): for UDP sockets, returns a tuple that pairs a string of returned data with the address from which it was just sent
            # mysocket.sendto(data, address): an unconnected UDP port uses this method to send a data packet to a particular remote address
    4.3 Five Socket Coordinates
        * The steps involved in creating and deploying a socket:
            # Address family choice: names what kind of network you want to talk to. All choices (seen with import socket; print dir(socket)) that start with AF_ are address families. IP operations all belong to the AF_INET family. AF_UNIX is also useful, in that it provides socket communications between programs on the same machine, without involving the full IP stack.
            # Socket type: there are packet based sockets, SOCK_DGRAM, and stream sockets with flow control, SOCK_STREAM. Most address families support one or both.
            # Protocol: rarely used, since the address family and the socket type typically narrow your choices toa  single major protocol.
            # IP Address
            # UDP or TCP port number
    4.4 IPv6
        * The address family for IPv6 is AF_INET6
        * You can test for whether your platform supports IPv6 with:
            
            import socket
            has_it = socket.has_ipv6
            
        * Though that doesn't tell you whether there is an actual IPv6 interface up and running
        * Differences for your code with IPv6:
            # Sockets have to be prepared to have the family AF_INET6
            # Socket names aren't just address+port, they also include "flow" info and "scope" identifier
            # The IP numbers are differently formatted
    4.5 Modern Address Resolution
        * The getaddrinfo() function is in the socket module, and deals with address parsing
        * Basic usage:
            
            from pprint import pprint
            infolist = socket.getaddrinfo('gatech.edu', 'www')
            pprint(infolist)
            ftpca = infolist[0]
            print ftpca[0:3]
            s = socket.socket(*ftpca[0:3])
            print ftpca[4]
            s.connect(ftpca[4])
            
        * getaddrinfo() returns information about what connections are possible to the supplied host and port
        * it allows you to supply symbols not only for the hostname, but also the port (www)
    4.6 Asking getaddrinfo() Where to Bind
        * If you want an address to give to bind(), either to create a server socket or to have a client connect to someone else from a predictable address, you call getaddrinfo() with None as the hostname but with the port number and socket type filled in.
        * Example for SMTP over TCP and DNS over UDP, with zeros as wildcards (these values will bind to every IPv4 and IPv6 interface on the local machine):
        
            from socket import getaddrinfo
            tcp_socket_info = getaddrinfo(None, 'smtp', 0, socket.SOCK_STREAM, 0, socket.AI_PASSIVE)
            udp_socket_info = getaddrinfo(None, 53, 0, socket.SOCK_DGRAM, 0, socket.AI_PASSIVE)
            
        * To bind() to a particular IP address that the local machine holds, omit the AI_PASSIVE flag and specify the hostname:
        
            getaddrinfo('127.0.0.1', 'smtp', 0, socket.SOCK_STREAM, 0)
            getaddrinfo('localhost', 'smtp', 0, socket.SOCK_STREAM, 0)
            
    4.7 Asking getaddrinfo() About Services
        * The typical use case for getaddrinfo(), other than requesting info to bind a socket, is to specify the AI_ADDRCONFIG flag when preparing to use connect() or sendto().
        * Typical usage:
        
            getaddrinfo('ftp.kernel.org', 
                        'ftp', 
                        0, 
                        socket.SOCK_STREAM, 
                        0, 
                        socket.AI_ADDRCONFIG | socket.AI_V4MAPPED)
                        
        * Multiple addresses may be returned--typically best to use the first one listed.
        * If you supply the final two args, you get only those access methods that you can actually use from your current network. To see all connection methods the host accepts, use:
        
            getaddrinfo('iana.org', 'www', 0, socket.SOCK_STREAM, 0)
            
    4.8 Asking getaddrinfo() for Pretty Hostnames
        * To get a more readable hostname to display to the user or record in a log file, run getaddrinfo() with the AI_CANONNAME flag on. The fourth item of any tuple it returns will be the canonical name:
        
            getaddrinfo('iana.org', 'www', 0, socket.SOCK_STREAM, 0, socket.AI_ADDRCONFIG | socket.AI_V4MAPPED | socket.AI_CANONNAME)
            
        * You can supply getaddrinfo() with the attributes of a socket that is already connected to a remote peer, and get a canonical name returned:
        
            mysock = old_sock.accept()
            addr, port = mysock.getpeername()
            getaddrinfo(addr, port, mysock.family, mysock.type, mysock.proto, socket.AI_CANONNAME)
            
    4.9 Other getaddrinfo() Flags
        * Flags may vary somewhat by OS
        * Common cross-platform flags:
            # AI_ALL: combine with AI_V4MAPPED (which maps v4 addresses to v6 if possible) to have every address known (v4 and v6) for the host returned.
            # AI_NUMERICHOST: turns off any attempt to interpret the hostname parameter as a textual hostname, only tries to interpret it as a literal v4 or v6 hostname.  Much faster, as no DNS calls get made.
            # AI_NUMERICSERV: turns off symbolic port names like www, insists numeric ports be used.
        * It is not necessary to worry about specifically dealing with domain names that have UTF8 characters--getaddrinfo() can take a unicode string as the hostname and deal with converting it as necessary.
    4.10 Primitive Name Service Routines
        * To find out the hostname of the current machine:
        
            socket.gethostname()
            socket.getfqdn()
            
        * To convert between IPv4 hostnames and IP addresses:
        
            socket.gethostbyname('cern.ch')
            socket.gethostbyaddr('137.138.144.169')
            
        * To look up protocol numbers and ports using symbolic names:
        
            socket.getprotobyname('UDP')
            socket.getservbyname('www')
            socket.getservbyport(80)
            
    4.11 Using getsockaddr() in Your Own Code
        * Example script:
        
            import socket, sys 
            
            if len(sys.argv) != 2:
                print >>sys.stderr, 'usage: www_ping.py <hostname_or_ip>'
                sys.exit(2)
            
            hostname_or_ip = sys.argv[1]
            
            try:
                infolist = socket.getaddrinfo(
                    hostname_or_ip, 'www', 0, socket.SOCK_STREAM, 0, socket.AI_ADDRCONFIG | socket.AI_V4MAPPED | socket.AI_CANONNAME,
                    )
            except socket.gaierror, e:
                print 'Name service failure:', e.args[1]
                sys.exit(1)
            
            info = infolist[0]
            socket_args = info[0:3]
            address = info[4]
            s = socket.socket(*socket_args)
            try:
                s.connect(address)
            except socket.error, e:
                print 'Network failure:', e.args[1]
            else:
                print 'Success: host', info[3], 'is listening on port 80'
        
    4.12 Better Living Through Paranoia
        * You can't necessarily trust that a hostname points to what you think it does.
        * Example script to check whether a hostname works forward and backward:
                    
            import socket, sys 
            
            if len(sys.argv) != 2:
                print >>sys.stderr, 'usage: forward_reverse.py <hostname>'
                sys.exit(2)
            hostname = sys.argv[1]
            
            try:
                infolist = socket.getaddrinfo(
                    hostname, 0, 0, socket.SOCK_STREAM, 0,
                    socket.AI_ADDRCONFIG | socket.AI_V4MAPPED | socket.AI_CANONNAME,
                    )
            except socket.gaierror, e:
                print 'Forward name service failure:', e.args[1]
                sys.exit(1)
            
            info       = infolist[0] # choose the first address
            canonical  = info[3]
            socketname = info[4]
            ip         = socketname[0]
            
            if not canonical:
                print 'WARNING! The IP address', ip, 'has no reverse name'
                sys.exit(1)
            
            print hostname, 'has IP address', ip
            print ip, 'has the canonical hostname', canonical
            
            forward = hostname.lower().split('.')
            reverse = canonical.lower().split('.')
            
            if forward == reverse:
                print 'Wow, the names agree completely!'
                sys.exit(0)
            
            # Truncate the domain names and compare
            length = min(len(forward), len(reverse))
            if (forward[-length:] == reverse[-length:]
                or (len(forward) == len(reverse)
                    and forward[-length+1:] == reverse[-length+1:]
                    and len(forward[-2]) > 2)): # avoid thinking '.co.uk' means a match
                print 'The forward and reverse names have a lot in common'
            else:
                print 'WARNING! The reverse name belongs to a different organization'
        
    4.13 A Sketch of How DNS Works
        * Turns hostnames into IP addresses
        * Based on RFC 1035 (1987)
        * Runs on TCP/IP and UDP/IP
        * Default port: 53
        * Libraries: PyDNS, dnspython
        * Non-DNS sources (/etc/hosts) will be consulted first
        * The DNS server runs a query for your domain starting with the TLD authorities
        * Eventually it resolves to the nameserver authority for the domain itself
        * You get back the IP address of their nameservers
    4.14 Why Not to Use DNS
        * Unless you specifically need DNS, it's more efficient to use a system supported lookup mechanism for getaddrinfo()
        * Using DNS directly may cause some local network functionality not to work
        * DNS will bypass local caching that the OS does for routing
        * The local system will probably have an OS specific DNS implementation you'll need to learn if you want to use DNS directly in your particular environment
        * If you don't use the local DNS config, you don't get the same caching as everything else on the system
        * If you make raw DNS calls yourself, you have to follow the upgrade path for your system's DNS implementation, to avoid bit rot
        * There's no DNS built into the python standard library
    4.15 Why to Use DNS
        * If you are a mail server, or a client trying to send mail directly to recipients without needing to run a local mail relay, and you want to look up MX records
        * There are two good third party DNS libraries for python: pydns and dnspython
        * Install pydns:
        
            $ virtualenv --no-site-packages dns; cd dns
            $ pip install pydns
            
        * Sample DNS script:
        
            import sys, DNS
            
            if len(sys.argv) != 2:
                print >>sys.stderr, 'usage: dns_basic.py <hostname>'
                sys.exit(2)
            
            DNS.DiscoverNameServers()
            request = DNS.Request()
            for qt in DNS.Type.A, DNS.Type.AAAA, DNS.Type.CNAME, DNS.Type.MX, DNS.Type.NS:
                reply = request.req(name=sys.argv[1], qtype=qt)
                for answer in reply.answers:
                    print answer['name'], answer['classstr'], answer['typename'], repr(answer['data'])
        
    4.16 Resolving Mail Domains
        * Rules for resolving MX domains are in RFC 5321
        * Briefly, the rules:
            # If MX records exist, you must try to contact those SMTP servers and return an error (or retry) if none of them will accept the message.
            # If no MX records exist, but an A or an AAAA record is provided, you may try an SMTP connection to that address.
            # If no MX, A or AAAA records exist, but a CNAME is specified, the domain name it supplies should be searched for MX or A records using the same rules
        * Basic implementation of those rules:

            import sys, DNS
            
            if len(sys.argv) != 2:
                print >>sys.stderr, 'usage: dns_mx.py <hostname>'
                sys.exit(2)
            
            def resolve_hostname(hostname, indent=0):
                """ Print an A or AAAA record for `hostname`; follow CNAMEs if necessary. """
                indent = indent + 4
                istr = ' ' * indent
                request = DNS.Request()
                reply = request.req(name=sys.argv[1], qtype=DNS.Type.A)
                if reply.answers:
                    for answer in reply.answers:
                        print istr, 'Hostname', hostname, '= A', answer['data']
                        return
                reply = request.req(name=sys.argv[1], qtype=DNS.Type.AAAA)
                if reply.answers:
                    for answer in reply.answers:
                        print istr, 'Hostname', hostname, '= AAAA', answer['data']
                        return
                reply = request.req(name=sys.argv[1], qtype=DNS.Type.CNAME)
                if reply.answers:
                    cname = reply.answers[0]['data']
                    print istr, 'Hostname', hostname, 'is an alias for', cname
                    resolve_hostname(cname, indent)
                    return
                print istr, 'ERROR: no records for', hostname
            
            def resolve_email_domain(domain):
                """ Print mail server IP addresses for an email address @ `domain`. """
                request = DNS.Request()
                reply = request.req(name=sys.argv[1], qtype=DNS.Type.MX)
                if reply.answers:
                    print 'The domain %r has explicit MX records!' % (domain,)
                    print 'Try the servers in this order:'
                    datalist = [ answer['data'] for answer in reply.answers ]
                    datalist.sort()
                    for data in datalist:
                        priority = data[0]
                        hostname = data[1]
                        print 'Priority:', priority, '  Hostname:', hostname
                        resolve_hostname(hostname)
                else:
                    print 'Drat, this domain has no explicit MX records'
                    print 'We will have to try resolving it as an A, AAAA, or CNAME'
                    resolve_hostname(domain)
            
            if __name__ == '__main__':
                DNS.DiscoverNameServers()
                resolve_email_domain(sys.argv[1])        
        
    4.17 Zeroconf and Dynamic DNS
        * The Zeroconf standard lets computers on a network discover each other without a central DNS or DHCP server.
        * Zeroconf includes "multicast DNS (mDNS)", which lets machines on the local network answer when another machine needs to resolve a hostname.
        * Dynamic DNS services are internet sites built to serve users whose machines are regularly changing their IP address. Run by dyndns.com, lets you do dynamic resolution via an API that you can sign into.
        
Chapter 5: Network Data and Network Errors
    5.1 Text and Encodings
        * The standard library 'codecs' package has the list of usable encodings in python
        * Unicode strings have encode() and decode() methods
        * It is dangerous to decode partial messages when using a multi-byte encoding, since a chunk may break across bytes.
    5.2 Network Byte Order
        * Different machines have had different ideas about the order in which to store binary numbers:
            # Big-endian computers (older SPARC machines, for example) put the most significant byte first, as when we store decimal digits
            # Little-endian computers (x86 and descendants) put the least significant byte first
        * Viewing the -endian difference is possible with the struct package:
        
            >>> import struct
            >>> struct.pack('<i', 4253)
            '\x9d\x10\x00\x00'
            >>> struct.pack('>i', 4253)
            '\x00\x00\x10\x9d'
            
        * The network byte order is big-endian. When you prep binary data to transmit on the network:
            # Use the struct module to produce binary data for transmission and to unpack it on arrival
            # Select network byte order with the '!' prefix if the data format is up to you.
            # If someone else has designed the protocol and specified little-endian, then you will have to use '<' instead
            # Always test struct to see how it lays out your data compared to the specification for the protocol you are speaking; note that 'x' characters in the packing format string can be used to insert padding bytes.
    5.3 Framing and Quoting
        * Framing is how you delimit messages so the receiver can tell where one ends and the next begins
        * One method is to have the receiver repeatedly call recv() until the sender closes the socket.  You only send a single stream of data in this method.
        * Example of this sort of single stream framing:
        
            import socket, sys
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            
            HOST = sys.argv.pop() if len(sys.argv) == 3 else '127.0.0.1'
            PORT = 1060
            
            if sys.argv[1:] == ['server']:
                s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                s.bind((HOST, PORT))
                s.listen(1)
                print 'Listening at', s.getsockname()
                sc, sockname = s.accept()
                print 'Accepted connection from', sockname
                sc.shutdown(socket.SHUT_WR)
                message = ''
                while True: 
                    more = sc.recv(8192)
                    if not more:
                        break
                    message += more
                print 'Done receiving the mssage; it says:'
                print message
                sc.close()
                s.close()
            
            elif sys.argv[1:] == ['client']:
                s.connect((HOST, PORT))
                s.shutdown(socket.SHUT_RD)
                s.sendall('Beautiful is better than ugly.\n')
                s.sendall('Explicit is better than implicit.\n')
                s.sendall('Simple is better than complex.\n')
                s.close()
            
            else:
                print >>sys.stderr, 'usage: streamer.py server|client [host]'
        
        * A second pattern is to do one stream from client to server, then one from server to client in response, then shut the sockets.
        * A third pattern is to use fixed length messages.
        * A fourth is to delimit messages with special characters. The recv() loop would wait until the reply string it was accumulating contained the delimiter indicating the end of the message.
        * If you're sending arbitrary data, a delimiter is a problem--so you use quoting. That's a pain, since you have to remove escape sequences and do careful parsing.
        * Fifth pattern: prefix easy message with its length. Once the length has been read and decoded by the receiver, the receiver can enter a loop and call() repeatedly until the whole message has arrived.
        * Sixth pattern for cases where you don't know message length. Send several blocks of data that are each prefixed with their length. Each chunk of new data is labeled with length and placed on the outgoing stream. When the end arrives, the sender can emit an agreed upon signal--perhaps a length field giving the number zero.
        * Simple example of the sixth pattern:

            import socket, struct, sys
            
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            
            HOST = sys.argv.pop() if len(sys.argv) == 3 else '127.0.0.1'
            PORT = 1060
            format = struct.Struct('!I') # for messages up to 2**32 - 1 in length
            
            def recvall(sock, length):
                data = ''
                while len(data) < length:
                    more = sock.recv(length - len(data))
                    if not more:
                        raise EOFError('socket closed %d bytes into a %d-byte message'
                                          % (len(data), length))
                    data += more
                return data
            
            def get(sock):
                lendata = recvall(sock, format.size)
                (length,) = format.unpack(lendata)
                return recvall(sock, length)
            
            def put(sock, message):
                sock.send(format.pack(len(message)) + message)
            
            if sys.argv[1:] == ['server']:
                s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                s.bind((HOST, PORT))
                s.listen(1)
                print 'Listening at', s.getsockname()
                sc, sockname = s.accept()
                print 'Accepted connection from', sockname
                sc.shutdown(socket.SHUT_WR)
                while True:
                    message = get(sc)
                    if not message:
                        break
                    print 'Message says:', repr(message)
                sc.close()
                s.close()
            
            elif sys.argv[1:] == ['client']:
                s.connect((HOST, PORT))
                s.shutdown(socket.SHUT_RD)
                put(s, 'Beautiful is better than ugly.\n')
                put(s, 'Explicit is better than implicit.\n')
                put(s, 'Simple is better than complex.\n')
                put(s, '')
                s.close()
            
            else:
                print >>sys.stderr, 'usage: streamer.py server|client [host]'
        
    5.4 Pickles and Self-Delimiting Formats
        * Some kinds of data have their own delimiting built in
        * If you used pickle, you'd want to treat it as file IO, and grab the pointer position after you'd run pickle.load() on it, to get everything after the pickle delimiter.
    5.5 XML, JSON, Etc.
        * Neither XML nor JSON support framing, so you have to figure out a way to deal with that over the network to use them
        * JSON isn't good for dealing with raw binary data
        * XML is better for documents
        * Other formats to consider: Google Protocol Buffers
    5.6 Compression
        * Network time is more overhead than processor time
        * zlib is available through the python standard library
        * zlib is self-framing
        * You could conceivably pass zlib'd data back and forth, and watch for dataobject.unused_data to become non-empty, if you passed extra characters at the end of the compressed data
    5.7 Network Exceptions
        * There are relatively few exceptions from socket operations that can actually impact your code.
        * Socket specific exceptions:
            # socket.gaierror: raised when getaddrinfo() can't find a name or service you ask about
            # socket.error: raised for nearly every failure that can happen at any stage in a network transmission. It's a subclass of IOError
            # socket.timeout: raised if you or a library you're using decides to set a timeout on a socket rather than wait forever for a send() or recv() to complete
        * Some higher level libraries will show you socket exceptions, some won't.
    5.8 Handling Exceptions
        * Four basic approaches:
            # Don't handle exceptions at all.
            # Wrapping the network errors in an exception of your own.
            # Wrap a try...except around every network call you make and print out an error message in its place.
            # Have big exception handlers that cover lots of code.  (Recommended approach.)
        
Chapter 6: TLS and SSL
    6.1 Computer Security
        * TLS: Transport Layer Security
        * SSL: Secure Sockets Layer
        * First: Have thorough tests.  Use the 'coverage' tool to measure how much of your code is being tested. When a module is difficult to test, it's a good signal that the module has too many hard-coded entanglements with other parts of the code.
        * Second: write as little code as possible; rely on well tested third party code.
        * Third: use a high level language like Python.
        * Fourth: learn as much as possible about the problem domain if many people have covered it before you.
        * Fifth: focus on the edges of your code where it interacts with data from outside.
    6.2 IP Access Rules
        * There is not really a good host filtering mechanism built into Python.
        * Two suggested reasons:
            # many modern sysadmins use firewalls to limit remote host access
            # IP address restrictions aren't actually effective as an ultimate security measure.
        * If you do want to impose IP access control, examine the IP address returned by the accept() method on the listening socket.
    6.3 Cleartext on the Network
        * Network sniffing via tcpdump and wireshark (among others) is problematic for cleartext.
        * Problematic stuff that can be sent cleartext:
            # credentials
            # log messages
            # db server info
        * You can get man-in-the-middle attacked via DNS cache poisoning if your traffic is in the clear.
    6.4 TLS Encrypts Your Conversations
        * Uses public key crypto to start each conversation
        * Server sends a public key to the client, client sends back a suitable symmetric key, encrypted with the public key.
        * Further traffic is encrypted with the symmetric key.
    6.5 TLS Verifies Identities
        * Typically the public key that servers offer to clients is backed by a certificate authority
        * Thereby man-in-the-middle attacks are defeated
        * Clients learn about CA certs via configuration files called via your SSL library
        * If you're doing all the connections internally, you can either be your own CA or skip the CA and use self-signed certs
    6.6 Supporting TLS in Python
        * You start a TLS connection by turning control of a socket over to an SSL library, and stop using the raw socket.
        * Both client and server should turn their sockets over to SSL at the same time
        * General approaches to doing SSL in python:
            # Use the ssl package that comes with python 3.2
            # Use the ssl package with python 2.6, and also the backports.ssl_match_hostname distro
            # Use a third party library like M2Crypto
    6.7 The Standard SSL Module
        * Example script for doing SSL cert matching:
        
            import os, socket, ssl, sys
            from backports.ssl_match_hostname import match_hostname, CertificateError
            
            try:
                script_name, hostname = sys.argv
            except ValueError:
                print >>sys.stderr, 'usage: sslclient.py <hostname>'
                sys.exit(2)
            
            # First we connect, as usual, with a socket
            
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.connect((hostname, 443))
            
            # Next, we turn the socket over to the ssl library
            
            ca_certs_path = os.path.join(os.path.dirname(script_name), 'certfiles.crt')
            sslsock = ssl.wrap_socket(sock, ssl_version=ssl.PROTOCOL_SSLv3, 
                                            cert_reqs=ss.CERT_REQUIRED, 
                                            ca_certs=ca_certs_path)
            
            # Does the cert that the server proferred *really* match the
            # hostname to which we are trying to connect? We need to check.
            
            try:
                match_hostname(sslsock.getpeercert(), hostname)
            except CertificateError, ce:
                print 'Certificate error:', str(ce)
                sys.exit(1)
            
            # From here on, our sslsock works like a normal socket. We can, for
            # example, make an impromptu HTTP call.
            
            sslsock.sendall('GET / HTTP/1.0\r\n\r\n')
            result = sslsock.makefile().read()
            sslsock.close()
            print 'The document https://%s/ is %d bytes long' % (hostname, len(result))

    6.8 Loose Ends
        * Use this as a loose guide, read up to date security docs.
        
Chapter 7: Server Architecture
    * Options for server design:
        # Basic design would be a single set of python instructions that served one client at a time
        # You can rewrite that into an event-driven style that can accept several client connections at once and then answer whichever one is ready for an answer next
        # Or run several copies of a single client server in separate threads or processes
        # Once you're leveraging the CPU effectively, you'll want to do load balancing
    7.1 Daemons and Logging
        * Official purpose of becoming a daemon process is so that a server can run independently of the terminal window and user session used to launch it.
        * Preferred method is to write a python program and then use the supervisord daemon to start and monitor the service.  Found at http://supervisord.org
        * There's also python-daemon from the Package Index
        * Under supervisord, output and error can be saved as rotated log files, but otherwise you have to make provision for writing your own logs.
        * Avoid the syslog module, and use the logging module instead.
        * Simplest pattern is:
            
            import logging
            log = logging.getLogger(__name__)
            ...
            log.error('the system is down')
            
    7.2 Our Example: Sir Launcelot
        * Simple network service to do a question and answer
        * Constants and methods file, launcelot.py:
        
            import socket, sys
            
            PORT = 1060
            qa = (('What is your name?', 'My name is Sir Launcelot of Camelot.'),
                  ('What is your quest?', 'To seek the Holy Grail.'),
                  ('What is your favorite color?', 'Blue.'))
            qadict = dict(qa)
            
            def recv_until(sock, suffix):
                message = ''
                while not message.endswith(suffix):
                    data = sock.recv(4096)
                    if not data:
                        raise EOFError('socket closed before we saw %r' % suffix)
                        message += data
                    return message
            
            def setup():
                if len(sys.argv) != 2:
                    print >>sys.stderr, 'usage: %s interface' % sys.argv[0]
                    exit(2)
                interface = sys.argv[1]
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                sock.bind((interface, PORT))
                sock.listen(128)
                print 'Ready and listening at %r port %d' % (interface, PORT)
                return sock

        * Server file, server_simple.py:            
            
            import socket, sys
            
            PORT = 1060
            qa = (('What is your name?', 'My name is Sir Launcelot of Camelot.'),
                  ('What is your quest?', 'To seek the Holy Grail.'),
                  ('What is your favorite color?', 'Blue.'))
            qadict = dict(qa)
            
            def recv_until(sock, suffix):
                message = ''
                while not message.endswith(suffix):
                    data = sock.recv(4096)
                    if not data:
                        raise EOFError('socket closed before we saw %r' % suffix)
                        message += data
                    return message
            
            def setup():
                if len(sys.argv) != 2:
                    print >>sys.stderr, 'usage: %s interface' % sys.argv[0]
                    exit(2)
                interface = sys.argv[1]
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
                sock.bind((interface, PORT))
                sock.listen(128)
                print 'Ready and listening at %r port %d' % (interface, PORT)
                return sock
            
    7.3 An Elementary Client
        * Simple client for the launcelot protocol:
                    
            import socket, sys, launcelot
            
            def client(hostname, port):
                s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                s.connect((hostname, port))
                s.sendall(launcelot.qa[0][0])
                answer1 = launcelot.recv_until(s, '.')
                s.sendall(launcelot[1][0])
                answer2 = launcelot.recv_until(s, '.')
                s.sendall(launcelot[2][0])
                answer3 = launcelot.recv_until(s, '.')
                s.close()
                print answer1
                print answer2
                print answer3
            
            if __name__ == '__main__':
                if not 2 < len(sys.argv) <= 3:
                    print >>sys.stderr, 'usage: client.py hostname [port]'
                    sys.exit(2)
                port = int(sys.argv[2]) if len(sys.argv) > 2 else launcelot.PORT
                client(sys.argv[1], port)        
        
    7.4 The Waiting Game
        * Difficult to benchmark client and server at the same time--to simulate real world network time, answer is to run both on a single machine, but send the traffic out and back through a proxy machine elsewhere.
        * 
        
TODO: Finish chapter 7.
        
Chapter 8: Caches, Message Queues, and Map-Reduce
    * Looking at tools that:
        # Is a well-written service that solves a particular problem
        # Solve problems that tend to be internal to an organization
        # Are generally agnostic about the data they carry for you
    8.1 Using Memcached
        * memcached is the memory cache daemon
        * Procedures for using memcached:
            # Run a memached daemon on every server with spare memory
            # Make a list of the IP addresses and port numbers of your new daemons
            # Distribute that list to all the clients who will use the service
            # Client programs now have access to a broad, fast, key-value cache
            # Cache operates on a least-recently-used basis, dropping old, unused items
        * Page with python memcached clients: http://code.google.com/p/memcached/wiki/Clients
        * python-memcached is written in pure python
        * Example usage:
        
            import memcache, random, time, timeit
            mc = memcache.Client(['127.0.0.1:11211'])
            
            def compute_square(n):
                value = mc.get('sq:%d' % n)
                if value is None:
                    time.sleep(0.001) # pretend this is an expensive op
                    value = n * n
                    mc.set('sq:%d' % n, value)
                return value
            
            def make_request():
                compute_square(random.randint(0, 5000))
            
            print 'Ten successive runs:',
            for i in range(1,11):
                print '%.2fs' % timeit.timeit(make_request, number=2000),
            print

        * What kind of data to cache? Many people cache lowest level of an expensive call, like the db response. Can also be good to cache higher level stuff like data structures, HTML snippets, entire web pages.
        * General points about memcached:
            # Keys have to be unique, so devs typically use prefixes and encodings to keep distinct the classes of objects they're storing: user:19, mypage:/node/14, etc.
            # Keys can only be 250 chars
            # Memcached is a cache and therefore ephemeral: based in RAM, clears on restart.
            # Make sure your cache doesn't return data that's too old to be accurate.
            # Set expiration dates if you need to
            # Actively invalidate cache entries if necessary
            # Rewrite/replace entries that become invalid
        * Decorators are a popular way to add caching in Python
        * Choose a data format for storing your data, like pickle or json
    8.2 Memcached and Sharding
        * Memcached supports sharding data storage across multiple servers
        * All memcached servers run a single hashing algorithm that can turn a key into an integer n that selects one of the servers from the list of available servers.
        * You don't want to shard your data using anything that exposes patterns that are in the data itself, like breaking on alphabetical order, because the distribution (and therefore the load) will be non-random.
    8.3 Message Queues
        * Message queues transmit data chunks atomically
        * Multiple topologies are possible (above and beyond TCP/UDP):
            # Pipeline: producer submits messages to the queue, from which the consumer can receive them
            # Publisher-subscriber: Where a pipeline delivers each queued message to exactly 1 consumer, this delivers each message to all interested subscribers
            # Request-reply: Allows round-trip communication. Clients that make requests have typically had to stay connected until they get a reply, but this lets you allow a client to make a request, disconnect, and have the queue system give them a response when available.
    8.4 Using Message Queues from Python
        * Current favorite implementation seems to be using the Advanced Message Queuing Protocol (AMQP), via the RabbitMQ message broker, with a Python AMQP client library like Carrot.
        * Celery seems to be a current favorite with Django devs.
        * Alternative to AMQP using a central broker is to use 0MQ, the "Zero Message Queue."
        * Rather than having a centralized broker, 0MQ moves the messaging intelligence into each of the message client programs.
        * Summary of 0MQ: www.zeromq.org/docs:welcome-from-amqp
        * Example of spreading with 0MQ:
        
            import random, threading, time, zmq
            zcontext = zmq.Context()
            
            def fountain(url):
                """ Produces a steady stream of words. """
                zsock = zcontext.socket(zmq.PUSH)
                zsock.bind(url)
                words = [ w for w in dir(__builtins__) if w.islower() ]
                while True:
                    zsock.send(random.choice(words))
                    time.sleep(0.4)
            
            def responder(url, function):
                """ Performs a string operation on each word received. """
                zsock = zcontext.socket(zmq.REP)
                zsock.bind(url)
                while True:
                    word = zsock.recv()
                    zsock.send(function(word)) # send modified word back
            
            def processor(n, fountain_url, responder_urls):
                """ Read words as they are produced; process; print them. """
                zpullsock = zcontext.socket(zmq.PULL)
                zpullsock.connect(fountain_url)
                zreqsock = zcontext.socket(zmq.REQ)
                for url in responder_urls:
                    zreqsock.connect(url)
                while True:
                    word = zpullsock.recv()
                    zreqsock.send(word)
                    print n, zreqsock.recv()
            
            def start_thread(function, *args):
                thread = threading.Thread(target=function, args=args)
                thread.daemon = True
                thread.start()
            
            start_thread(fountain, 'tcp://127.0.0.1:6700')
            start_thread(responder, 'tcp://127.0.0.1:6701', str.upper)
            start_thread(responder, 'tcp://127.0.0.1:6702', str.lower)
            for n in range(3):
                start_thread(processor, n + 1, 'tcp://127.0.0.1:6700',
                     ['tcp://127.0.0.1:6701', 'tcp://127.0.0.1:6702'])
            time.sleep(30)
        
    8.5 How Message Queues Change Programming
        * Without message queues, programs tend to be about communicating between functions, and the through line of a program is about passing data from one function to another directly.
        * With message queues, program design may skew more towards components that may come from different languages, connected by a standardized message broker.
        * New functionality can be brought on as separate services
        * With a single address space and lots of python packages and libraries, the mechanisms for concurrency are locks, semaphores and shared data structures
        * Message services have a different model: small autonomous services attached to a common queue, letting the queue handle message transport between processes
    8.6 Map-Reduce
        * The purpose of a map-reduce system is eliminating the problem of distributing code and data across multiple, clustered servers.
        * Two main reasons for distributing a computation:
            # A task requires a lot of CPU
            # A task requires a large, distributed data set
        * A map-reduce framework takes responsibility for both distributing tasks and assembling an answer, by imposing structure on the processing code submitted:
            # Task must be broken into two operations: "map" and "reduce"
            # Mapping operation should be prepped to run once on a slice of the overal data set, and produce a tally, table, or response that summarizes findings for that slice
            # Reduce operation is exposed to the outputs of the mapping functions, combining them into an overall answer
            # Efficient implementations will recursively run the reduce op until all intermediate steps are combined.
        
Chapter 9: HTTP
    * HTTP is text based
    * Headers are separated with \r\n
    * Headers end when you see two EOL sequences with nothing between them: \r\n\r\n
    * Everything after that is the document, not the headers
    * Chapter focuses on the Python Standard Library urllib2 module
    9.1 URL Anatomy
        * Made of:
            # Protocol
            # Hostname
            # Port
            # Resource + Parameters
        * Characters beyond the alphanumerics and $-_.+!'(), must be URL encoded
        * Most important url parsing routines:
            
            from urlparse import urlparse, urldefrag, parse_qs, parse_qsl
            
    9.2 Relative URLs
        * Two cases, both resolved with urlparse.urljoin:
        
            urlparse.urljoin('http://www.python.org/psf/', 'grants')
              produces: http://www/python.org/psf/grants/
              
            urlparse.urljoin('http://www.python.org/psf/', '../news/')
              produces: http://www.python.org/news/
              
    9.3 Instrumenting urllib2
        * Example of an HTTP request and response, printing all headers:
        
            import StringIO, httplib, urllib2
            
            class VerboseHTTPResponse(httplib.HTTPResponse):
                def _read_status(self):
                    s = self.fp.read()
                    print '-' * 20, 'Response', '-' * 20
                    print s.split('\r\n\r\n')[0]
                    return httplib.HTTPResponse._read_status(self)
            
            class VerboseHTTPConnection(httplib.HTTPConnection):
                response_class = VerboseHTTPResponse
                def send(self, s):
                    print '-' * 50
                    print s.strip()
                    httplib.HTTPConnection.send(self, s)
            
            class VerboseHTTPHandler(urllib2.HTTPHandler):
                def http_open(self, req):
                    return self.do_open(VerboseHTTPConnection, req)

        * urllib2 lets you bypass urlopen() and build an opener full of handler classes
        * The above provides a handler by customizing the normal HTTPHandler
    9.4 The GET Method
        * GET is the first thing transmitted as part of an HTTP request, followed by request headers
        * The openers open() method returns an information object that lets us examine the result of the GET
        * Info object is dictionary-like
        * Info object can act as a file, so you can either pull it in pieces with read(n) or readline(), or pull the entire data stream into memory as a string
        * Example usage:
        
            >>> info = opener.open('http://www.ietf.org/rfc/rfc2616.txt')
            >>> info.code
            200
            >>> info.msg
            'OK'
            >>> info.headers.keys()
            ['content-length', 'accept-ranges', 'vary', 'server', 'last-modified', 'connection', 'etag', 'date', 'content-type']
        
    9.5 The Host Header
        * URL location is now included in every HTTP request, in the headers under Host
    9.6 Codes, Errors, and Redirection
        * Various HTTP responses:
            # 200 OK: Request succeeded
            # 301 Moved Permanently: Rewrite bookmarks to use the new URL
            # 303 See Other: Retrieve another url now, but continue to use this URL 
            # 304 Not Modified: Client has an up-to-date copy of the resource
            # 307 Temporary Redirect: Like a 303, except for POST or PUT, where it means you must retry with another POST or PUT at the URL given
            # 404 Not Found
            # 500 Internal Server Error
            # 503 Service Unavailable: Temporary or transient service failure
        * urllib2 will automatically follow redirections
        * Return codes that can't be handled or are errors are raised as Python exceptions
        * Catch the exception to view the HTTP response--the exception can be queried for status code and message, and can be read from like an info object
    9.7 Payloads and Persistent Connections
        * HTTP/1.1 servers will keep a TCP connection open even after delivering a response
        * urllib2.HTTPConnection lets you take advantage of that
        * When you might make serveral requests to the same site, use a persistent connection:
        
            >>> import httplib
            >>> c = httplib.HTTPConnection('www.python.org')
            >>> c.request('GET', '/')
            >>> original_sock = c.sock
            >>> content = c.getresponse().read()
            >>> c.request('GET', '/about/')
            >>> c.sock is original_sock
            True

        * RFC 2616 defines a header named Connection: that can be used to explicitly indicate that a request is the last one that will be made on a socket.  If we insert it, we can force the HTTPConnection object to create a second socket when we ask for a second page:
        
            >>> import httplib
            >>> c = httplib.HTTPConnection('www.python.org')
            >>> c.request('GET', '/', headers={'Connection': 'close'})
            >>> original_sock = c.sock
            >>> content = c.getresponse().read()
            >>> c.request('GET', '/about/')
            >>> c.sock is original_sock
            False

    9.8 POST and Forms
        * POST hides request parameters
        * Should be used for requests that aren't building a dynamic page URL
        * POST request args are carried in the body of the request
        * We can do that explicitly by using urlencode to format the form parameters and then supply them as a second parameter to any of the urllib2 methods that open a URL
        * Example POST:
        
            >>> from verbose_handler import VerboseHTTPHandler
            >>> import urllib, urllib2
            >>> opener = urllib2.build_opener(VerboseHTTPHandler)
            >>> form = urllib.urlencode({'inputstring': 'Atlanta, GA'})
            >>> response = opener.open('http://forecast.weather.gov/zipcity.php', form)
            --------------------------------------------------
            POST /zipcity.php HTTP/1.1
            Accept-Encoding: identity
            Content-Length: 25
            Host: forecast.weather.gov
            Content-Type: application/x-www-form-urlencoded
            Connection: close
            User-Agent: Python-urllib/2.6
            --------------------------------------------------
            inputstring=Atlanta%2C+GA
            -------------------- Response --------------------
        
    9.9 Successful Form POSTs Should Always Redirect
        * Well designed user-facing POST forms always redirect to a page that shows the result of the action that was requested of the server.
        * One exception is an unsuccessful POST, which should display the form again, prefilled.
    9.10 POST and APIs
        * POST requests going to a non-web-browser program don't have to obey the above caveats, and don't have to use the x-www-form-urlencoded data format for input parameters.
    9.11 REST and More HTTP Methods
        * A lot of modern web services use additional methods, like PUT or DELETE
        * REST (Representational State Transfer) exists to take more advantage of HTTP
        * Very basic REST: 'nouns' of an API should live at URLs, 'verbs' like PUT, GET, POST, DELETE should be used to create, fetch, modify, and remove the documents at URLs
    9.12 Identifying User Agents and Web Servers
        * User-Agent header is option in the HTTP protocol
        * Sometimes used to distiguish browsers, or bots from browsers
        * You can tell urllib2 to spoof its user agent:
        
            >>> agent = 'Mozilla/5.0 (Windows; U; MSIE 7.0; Windows NT 6.0; en-US)'
            >>> request = urllib2.Request(url)
            >>> request.add_header('User-Agent', agent)
            >>> urllib2.urlopen(request).read()        

    9.13 Content Type Negotiation
        * Web servers tend to return content in one of:
            # text/html
            # text/plain
            # text/css
            # image/gif
            # image/jpeg
            # image/x-png
            # application/javascript
            # application/pdf
            # application/zip
        * Generic stream is application/octet-stream
        * If you find your Python web client is sophisticated enoguh to need content negotiation, read RFC 2616 and look at the headers Accept, Accept-Charset, Accept-Language, Accept-Encoding
    9.14 Compression
        * Web pages, CSS, and JS files are amenable to text compression
        * Clients can make servers aware they accept compressed documents via Accept-Encoding: gzip, and by setting the User-Agent to something the server is likely to recognize.
        * Check for returned type--many servers will ignore your Accept-Encoding header
    9.15 HTTP Caching
        * Servers typically add headers to responses that let the client process decide whether to cache each resource
        * Two basic mechanisms for client side caching:
            # Each HTTP response includes an Expires: header with a datetime string formatted the same way as the standard Date: header
            # Alternatively, you can use the Cache-Control header, which is formatted like:
            
                Cache-Control: max-age=3600, must-revalidate
                
        * Neither urllib2 nor mechanize support caching--if you need a local cache, you want httplib2 from the Python Package Index
    9.16 The HEAD Method
        * If you issue a HEAD request through httplib, you can check links for validity or to see if they've moved. The body of the response is generally empty.
    9.17 HTTPS Encryption
        * Encrypted URLs start with https:
        * Happens over port 443 instead of 80
        * Uses TLS
        * To use HTTPS from python, use an https: method in your URL:
        
            info = urllib2.urlopen('https://www.ietf.org/rfc/rfc2616.txt')
            
    9.18 HTTP Authentication
        * Can be used under python with HTTPBasicAuthHandler()
    9.19 Cookies
        * HTTP Responses sent by a server can include a number Set-cookie: headers that browsers will store. In subsequent requests, the browser will include a Cookie: header corresponding to each cookie that has been set.
        * Cookies alone can't assert identity--too easy to spoof. Two approaches:
            # server can store a random variable in the cookie that must match a back end copy
            # cookie can be a block of encrypted data that the server decrypts
        * Python Standard Library puts cookie logic into cookielib
        * Usage for an opener that knows about cookies:
        
            >>> import cookielib
            >>> import urllib2
            >>> cj = cookielib.CookieJar()
            >>> from verbose_handler import VerboseHTTPHandler
            >>> cookie_opener = urllib2.build_opener(VerboseHTTPHandler, urllib2.HTTPCookieProcessor(cj))
            >>> response = cookie_opener.open('http://www.google.com/')
            --------------------------------------------------
            GET / HTTP/1.1
            Accept-Encoding: identity
            Host: www.google.com
            Connection: close
            User-Agent: Python-urllib/2.6
            -------------------- Response --------------------
            HTTP/1.1 200 OK
            Date: Mon, 29 Aug 2011 23:42:58 GMT
            Expires: -1
            Cache-Control: private, max-age=0
            Content-Type: text/html; charset=ISO-8859-1
            Set-Cookie: PREF=ID=911822f5eb29267d:FF=0:TM=1314661378:LM=1314661378:S=uma3uv_lnP-0vhp3; expires=Wed, 28-Aug-2013 23:42:58 GMT; path=/; domain=.google.com
            Set-Cookie: NID=50=LOcAgutwXD477k6jT22-mhl20MlZfBmLgP3Sd4zC1EMV8s5EJVHKMzwjAVSjXus4SVnecOipibVjRFDUR7YRA_3JnaqDbk4Ya4iJyIhrPeAhP_eGy0E3ME5CaLA0rFjd; expires=Tue, 28-Feb-2012 23:42:58 GMT; path=/; domain=.google.com; HttpOnly
            Server: gws
            X-XSS-Protection: 1; mode=block
            Connection: close
            
        * You can choose to automatically store cookies in a file, so they survive from one Python session to the next. See cookielib docs.
    9.20 HTTP Session Hijacking
        * Cookies need to be protected to avoid session hijacking.
        * Either send all info over HTTPS, or serve non-authenticating stuff like CSS scripts from a different domain or path outside the jurisdiction of the session cookie.
        * Session cookies should always be marked secure, so browsers won't divulge them over insecure links.
    9.21 Cross-Site Scripting Attacks
        * The "same origin policy" is a security measure that says that scripts (like JS) can only make connections back to the site that served the web page, and not to other sites.
        * XSS attacks rely on things like finding fields on a page where the site will include snippets of user-provided data without properly escaping them, then using that to wedge open some part of the server.
        * Some techniques that help avoid XSS:
            # When processing a form that is supposed to submit a POST, disregard any GET parameters
            # Never support URLs that produce some side effect or perform an action simply through being the subject of a GET
            # In every form, include a hidden field with a secret value that must match for the submission to be valid.
    9.22 WebOb
        * WebOb is an additional python library for using HTTP request and response classes. It interfaces particularly well with WSGI.
        * More info here: http://pythonpaste.org/webob/
            
Chapter 10: Screen Scraping
    * Always check for an API before screen scraping
    * Check the TOS before scraping
    * Polite scrapers cache and aren't aggressive.
    10.1 Fetching Web Pages
        * Options for downloading content:
            # Use urllib2 or the lower level httplib to construct an HTTP request
            # Install mechanize and write a program that fills out and submits forms
            # To download and prase entire sites, look at the Scrapy project, http://scrapy.org
            # For dynamically built JS pages, you can use QtWebKit from PyQt4
            # If you need a browser to load a site, Selenium and Windmill test platforms have a way to drive a standard web browser from inside a python program
    10.2 Downloading Pages Through Form Submission
        * Find the form you need to submit to--only things that matter are the form and input fields.
        * Example program that submits a form:
        
            import urllib, urllib2
            
            data = urllib.urlencode({'inputstring': 'Phoenix, AZ'})
            info = urllib.urlopen('http://forecast.weather.gov/zipcity.php', data)
            content = info.read()
            open('phoenix.html', 'w').write(content)
        
        * Using mechanize to parse a page:
        
TODO: Finish Chapter 10

Chapter 11: Web Applications
    * Apps that face the internet require at least three big decisions:
        # What front end web server to use
        # How to link the server and the python application
        # How to spawn multiple copies of the python application code
        # Which libraries to use for common tasks like URL dispatch, db access and template rendering
    11.1 Web Servers and Python
        * Python doesn't run code in a thread-safe manner: only one thread can have the GIL at any point in time
        * That's alright because most web apps are actually a light front end between a user and a database
        * Two general approaches to running a python web app inside a collector of worker processes:
            # Apache can be combined with mod_wsgi to host a separate python interpreter in every Apache worker process
            # The webapp can be run inside either the flup server or the uWSGI server. Both will manage a pool of worker processes where each process hosts a Python interpreter running your application.
    11.2 Two Tiers
        * The vast majority of web requests break down into two types of resource request, static and dynamic.
        * Consequently, there are typically two tiers of servers in a production webapp:
            # First server faces users and their browsers, and serves high-volume static content directly from disk.
            # Second server is a framework to power the dynamic pages. Invoked only for pages that require it--often can only be reached with a request from the front end server.
    11.3 Choosing a Web Server
        * Couple of main options:
            # Apache HTTP Server
            # nginx
            # lighttpd
            # Cherokee
        * mod_wsgi has a daemon mode where it internall runs your python code inside a stack of dedicated server processes separate from apache
        * Strongly recommended: set up one of the three fast servers to serve static content, then use one of the following techniques to run python code behind them:
            # Use HTTP proxying so that your nginx, lighttpd or Cherokee front-end server delivers HTTP requests for dynamic web pages to a back end Apache instance running mod_wsgi
            # Use the FastCGI protocol or SCGI protocol to talk to a flup instance running your python code
            # Use the uwsgi protocol to talk to a uWSGI instance running your python code.
    11.4 WSGI
        * PEP 333 defines the Python Web Server Gateway Interface (WSGI)
        * Uses a single calling convention that every web server can implement
        * Developers don't write raw WSGI apps because web frameworks make everything easy
        * For illustration, here's a WSGI app:
        
            import cgi, base64
            from wsgiref.simple_server import make_server
            
            def page(content, *args):
                yield '<html><head><title>wsgi_app.py</title></head><body>'
                yield content % args
                yield '</body>'
            
            def simple_app(environ, start_response):
                gohome = '<br><a href="/">Return to the home page</a>'
                q = cgi.parse_qs(environ['QUERY_STRING'])
            
                if environ['PATH_INFO'] == '/':
                    if environ['REQUEST_METHOD'] != 'GET' or environ['QUERY_STRING']:
                        start_response('400 Bad Request', [('Content-Type', 'text/plain')])
                        return ['Error: the front page is not a form.']
                    start_response('200 OK', [('Content-Type', 'text/html')])
                    return page('Welcome! Enter a string <form action="encode">'
                                '<input name="mystring"><input type="submit"></form>')
                elif environ['PATH_INFO'] == '/encode':
                    if environ['REQUEST_METHOD'] != 'GET':
                        start_response('400 Bad Request', [('Content-Type', 'text/plain')])
                        return ['Error: this form does not support POST parameters.']
                    if 'mystring' not in q or not q['mystring'][0]:
                        start_response('400 Bad Request', [('Content-Type', 'text/plain')])
                        return ['Error: this form requires a "mystring" parameter']
                    my = q['mystring'][0]
                    start_response('200 OK', [('Content-Type', 'text/html')])
                    return page('<tt>%s</tt> base64 encoded is: <tt>%s</tt>' + gohome,
                                cgi.escape(repr(my)), cgi.escape(base64.b64encode(my)))
                else:
                    start_response('404 Not Found', [('Content-Type', 'text/plain')])
                    return ['That URL is not valid.']
            
            print 'Listening on localhost:8000'
            make_server('localhost', 8000, simple_app).serve_forever()
        
    11.5 WSGI Middleware
        * Wrappers (or "adapters") for WSGI will accept a request from a server, modify, adjust or record the request, then call a normal WSGI application with the modified environment.
        * Possibilities for WSGI Middleware include:
            # managing the routing for multiple applications that live under a single domain
            # dealing with user authentication at a level higher than the individual webapps
            # doing coherenet HTML themeing for multiple apps
            # Debugging app calls
        * Most python programmers don't really use WSGI middleware
        * Python web frameworks support WSGI, but it isn't the main mechanism for constructing python web applications
        * Three major competing approaches in the Python community for using modular components to build websites:
            # WSGI middleware approach, with a component stack where each talks to the other through WSGI
            # the Zope Toolkit, which uses formal Design Patterns like interfaces and factories, with WSGI adapters
            # Web frameworks that attempt to make it easy for third party functionality to be added to an application easily. Django being the foremost in this camp.
    11.6 Python Web Frameworks
        * Web frameworks let you step back from the details of HTTP, and write code that focuses on the "nouns" of web design.
        * Very simple framework usage, with bottle:
        
            import base64, bottle
            bottle.debug(True)
            app = bottle.Bottle()
            
            @app.route('/encode')
            @bottle.view('bottle_template.html')
            def encode():
                mystring = bottle.request.GET.get('mystring')
                if mystring is None:
                    bottle.abort(400, 'This form requres a "mystring" parameter"')
                return dict(mystring=mystring, myb=base64.b64encode(mystring))
            
            @app.route('/')
            @bottle.view('bottle_template.html')
            def index():
                return dict(mystring=None)
            
            bottle.run(app=app, host='localhost', port=8080)

        * With template:
        
            %#!/usr/bin/env python
            <html>
                <head>
                    <title>bottle_app.py</title>
                </head>
                <body>
                    %if mystring is None:
                        Welcome! Enter a string:
                        <form action="encode"><input name="mystring"><input type="submit"></form>
                    %else:
                        <tt>{{mystring}}</tt> base 64 encoded is: <tt>{{myb}}</tt><br />
                        <a href="/">Return to the home page.</a>
                    %end
                </body>
            </html>
        
    11.7 URL Dispatch Techniques
        * Some frameworks (bottle and flask) let you create small apps by decorating a series of callables with URL patterns
        * Things like Django, Pylons, Werkzeug, encourage defining URLs all in one place
        * Another approach is to define controllers, classes that represent some point in the URL hierarchy, and then write methods on the controller class that represent sub pages in that URL section
        * Zope + Plone are a whole 'nother thing
    11.8 Templates
        * Most frameworks expect pages to be produced by combining view code with a template
    11.9 Final Considerations
    11.10 Pure-Python Web Servers
        * The standard library includes SimpleHTTPServer, a subclass of BaseHTTPServer
        * Basically very quick, development-only servers
    11.11 CGI
        * CGI itself is to be avoided entirely--too expensive to spawn a process per web request
        * Essential features of CGI, should it come up:
            # Lots of HTTP request stuff got put into the unix environment vars
            # Data was passed from standard input, streamed to EOF
            # Script would produce content, by writing HTTP headers, a blank line, and a response body
        * If you have to use something behind an HTTP server that only supports CGI, use the CGIHandler module from the wsgiref standard library package
    11.12 mod_python
        * Lets you put a python interpreter inside every worker process spawned by apache
        * You specified a PythonHandler as an apache directive
        
TODO: Chapter 12: Email Composition and Decoding
TODO: Chapter 13: SMTP
TODO: Chapter 14: POP
TODO: Chapter 15: IMAP
TODO: Chapter 16: Telnet and SSH
TODO: Chapter 17: FTP
        
Chapter 18: RPC
    * Three features that mark a protocol as an example of Remote Procedure Calls:
        # Lacks strong semantics for the meaning of each call. Data isn't meaningful beyond being of a supported datatype--comprehension is up to the API you're using.
        # RPC mechanisms are a way to invoke methods, but don't define or limit them.
        # Using RPC should look more or less like any other function call, other than only passing it data, not live objects like a file handle.
    18.1 Features of RPC
        * There are limits on the kinds of data an RPC protocol will pass, usually imposed by the programming language that backs the API you're using.
        * The server can signal an exception, and typically the client RPC library will raise an exception itself to tell the client about the problem.
        * There is typically some level of introspection, in terms of finding out what calls are supported, what arguments they take, etc.
        * Has to have some addressing scheme for reaching out and connecting to a particular remote API.
        * Some RPC mechanisms support authentication, access control, or impersonation of user accounts.
    18.2 XML-RPC
        * Older protocol, but natively supported in Python
        * Runs natively under HTTP, no need for a separate protocol
        * Libraries: xmlrpclib, SimpleXMLRPCServer, DocXMLRPCServer
        * XML-RPC spec builds additional semantics on top of the plain XML document format
        * Simple XML-RPC Server:
        
            import operator, math
            from SimpleXMLRPCServer import SimpleXMLRPCServer
            
            def addtogether(*things):
                """ Add together everything in the list `things`. """
                return reduce(operator.add, things)
            
            def quadratic(a,b,c):
                """ Determine x values satisfying: a * x^2 + b * x + c == 0 """
                b24ac = math.sqrt(b*b - 4.0*a*c)
                return list(set([ (-b-b24ac) / 2.0*a,
                                  (-b+b24ac) / 2.0*a ]))
            
            def remote_repr(arg):
                """ Return the repr() rendering of the arg. """
                return arg
            
            server = SimpleXMLRPCServer(('127.0.0.1', 7001))
            server.register_introspection_functions()
            server.register_multicall_functions()
            server.register_function(addtogether)
            server.register_function(quadratic)
            server.register_function(remote_repr)
            print "Server ready"
            server.serve_forever()
        
        * An RPC service lives at a URL, so you don't need a dedicated port
        * If you have a port to spare, it's easy to put up an XML-RPC only server
        * Example introspection of the above server:
        
            import xmlrpclib
            proxy = xmlrpclib.ServerProxy('http://127.0.0.1:7001')
            
            print 'Here are the functions supported by this server:'
            for method_name in proxy.system.listMethods():
                if method_name.startswith('system.'):
                    continue
                signatures = proxy.system.methodSignature(method_name)
                if isinstance(signatures, list) and signatures:
                    for signature in signatures:
                        print '%s(%s)' % (method_name, signature)
                else:
                    print '%s(...)' % (method_name,)
            
                method_help = proxy.system.methodHelp(method_name)
                if method_help:
                    print '  ', method_help
        
        * Client example (produces a math domain error):
        
            import xmlrpclib
            proxy = xmlrpclib.ServerProxy('http://127.0.0.1:7001')
            print proxy.addtogether('x', 'y', 'z')
            print proxy.addtogether(20, 30, 4, 1)
            print proxy.quadratic(2, -4, 0)
            print proxy.quadratic(1, 2, 1)
            print proxy.remote_repr((1, 2.0, 'three'))
            print proxy.remote_repr([1, 2.0, 'three'])
            print proxy.remote_repr({'name': 'John', 'data': {'age': 20, 'sex': 'M'}})
            print proxy.quadratic(1,0,1)

        * To make several calls in a network round trip, use xmlrpclib.MultiCall:
        
            import xmlrpclib
            proxy = xmlrpclib.ServerProxy('http://127.0.0.1:7001')
            multicall = xmlrpclib.MultiCall(proxy)
            multicall.addtogether('a','b','c')
            multicall.quadratic(2, -4, 0)
            multicall.remote_repr([1, 2.0, 'three'])
            for answer in multicall():
                print answer
        
        * Three final points on XML-RPC:
            # Python's XML-RPC implements date values and None
            # Keyword arguments are not supported by XML-RPC
            # Dictionaries can only be passed if all their keys are strings
    18.3 JSON-RPC
        * Serializes data structures to strings in JSON syntax, which can be inflated by a browser via the eval() function
        * Makes data more compact, simplifies parsers and library code over XML
        * Not supported in the python standard library
        * Libraries are third party, including lovely.jsonrpc
        * Example JSON-RPC server:
        
            from wsgiref.simple_server import make_server
            import lovely.jsonrpc.dispatcher, lovely.jsonrpc.wsgi
            
            def lengths(*args):
                results = []
                for arg in args:
                    try:
                        arglen = len(arg)
                    except TypeError:
                        arglen = None
                    results.append((arglen, arg))
                return results
            
            dispatcher = lovely.jsonrpc.dispatcher.JSONRPCDispatcher()
            dispatcher.register_method(lengths)
            app = lovely.jsonrpc.wsgi.WSGIJSONRPCApplication({'': dispatcher})
            server = make_server('localhost', 7002, app)
            print "Starting server"
            while True:
                server.handle_request()
        
        * JSON-RPC Client:

            from lovely.jsonrpc import proxy
            proxy = proxy.ServerProxy('http://localhost:7002')
            print proxy.lengths((1,2,3), 27, {'Sirius': -1.46, 'Rigel': 0.12})
        
    18.4 Self-documenting Data
        * Think of dictionaries being sent across RPCs as being like the __dict__ attributes of your Python objects
        * {{'number': 1, 'symbol': 'H'}, ...} is better than {'1':'H', ...}
        * Code to go back and forth between condensed and expanded formats:
        
            >>> elements = {1: 'H', 2: 'He'}
            >>> t = [ {'number': key, 'symbol': elements[key]} for key in elements ]
            >>> t
            [{'symbol': 'H', 'number': 1}, {'symbol': 'He', 'number': 2}]
            >>> dict( (obj['number'], obj['symbol']) for obj in t )
            {1: 'H', 2: 'He'}
            
    18.5 Talking about Objects: Pyro and RPyC
        * Passing objects via RPC is really difficult
        * Walking objects to serialize them can be really expensive and slow
        * The general case problem is difficult, python program to python program it's not as bad
        * There are two python native RPC systems to mention: Pyro and RPyC
        * Pyro is at  http://ww.xs4all.nl/~irmen/pyro3/
        * Pyro is built on top of pickle, can send anything that is pickleable
    18.6 An RPyC Example
        * RPyC is here: http://rpyc.wikidot.com/
        * Takes a more sophisticated approach than using pickle, since pickle can accidentally pass arbitrary code to your program
        * Example client program in RPyC:

            import rpyc
            
            def noisy(string):
                print 'Noisy:', repr(string)
            
            proxy = rpyc.connect('localhost', 18861, config={'allow_public_attrs': True})
            fileobj = open('testfile.txt')
            linecount = proxy.root.line_counter(fileobj, noisy)
            print 'The number of lines in the file was', linecount

        * Example server in RPyC:
        
            import rpyc
            
            class MyService(rpyc.Service):
                def exposed_line_counter(self, fileobj, function):
                    for linenum, line in enumerate(fileobj.readlines()):
                        function(line)
                    return linenum + 1
            
            from rpyc.utils.server import ThreadedServer
            t = ThreadedServer(MyService, port = 18861)
            t.start()

        * RPyC code, instead of serializing and sending as much as possible, serializes only completely immutable items like python integers, floats, strings and tuples. For everything else, it passes an object name that lets the remote side reach back into the client to access attributes and invoke methods on live objects.
        * Lots of network traffic this way, possible delays
        * May require adjusting the permissions you give the server on connection, if you aren't comfortable with the blanket allow_public_attrs mode
    18.7 RPC, Web Frameworks, Message Queues
        * Three useful ways to look beyond simple examples of how to use RPC:
            # See if you can use WSGI to install an RPC service that you incorporate into a larger web project you're deploying. Running both your project and your service as WSGI servers beneath a URL filter lets you run both under the same hostname and port.
            # Instead of a dedicated RPC library, use the RPC stuff built into your web framework
            # Try sending RPC messages over an alternate transport that does a better job than the protocol's native transport of routing calls to servers that are ready to handle them. Message queues can be a great vehicle for RPC calls when you want to balance load over multiple servers.
    18.8 Recovering from Network Errors
        * Most services raise an exception on network failure
        * Tricks for writing code that delegates function calls across the network:
            # Distinguish exceptions in the remote code from problems with the protocol/network
            # Wrap large pieces of your code that have a solid semantic meaning in a high level error handler, instead of using lots of try...except blocks.
            # Be careful about the loss of exception detail across the network--KeyError or ValueError exceptions on the remote side can become an RPC specific error you have to inspect to find out what happened.
    18.9 Binary Options: Thrift and Protocol Buffers
        * You can use BSON to do binary transfer
        * The apache foundation is working on Thrift, an open source RPC system that gives more control over data types and parameters
        * Google Protocol Buffers are popular, though not a full RPC system
                
        
        
        
        
        
        
        