# Notes on Data Lake Architecture: Designing the Data Lake and Avoiding the Garbage Dump

By Bill Inmon, Technics Publications, April 2016

ISBN 9781634621175

# Introduction

* Data lake should be divided into sections (ponds):
    * Raw data
    * Analog data
    * Application data
    * Textual data
    * Archival data
* Post creation, ponds require conditioning for access and usability

# Chapter 1: Data Lakes

* Initial corp attempts at big data ingestion created lakes
* Lakes became one way--no way to get good stuff out
* Eventually become dumping grounds
* Problems with this:
    * Useful data becomes that much harder to find in the sea of noise
    * Metadata gets lost or is stored poorly
    * Relationships between data get lost / corrupted

# Chapter 2: Transforming the Data Lake

* Four basic ingredients in the transformation:
    * Metadata - data keys, etc
    * Integration Mapping - how data from one app relates to data from another
    * Context - Additonal metadata for textual sources (e.g., for corpus X, 'court' = 'tennis court', not 'legal court')
    * Metaprocess - how data was processed or will be processed; audit trail for transformations

# Chapter 3: Inside the Data Lake

* Three broad categories of data you find:
    * Analog data - typically machine generated measurement data
        * Typically data points that have accompanying metadata and metaprocess
    * Application data - generated by application execution and user transactions
        * Typically record-based
    * Textual data
        * Non-record, unstructured data in text format
        * Requires disambiguation to be standardized and usable
        * Disambiguation accomplishes:
            * structures the data
            * defines and associates contextual data
* Additional classification: repetitive vs. non-repetitive data
    * Textual data is non-repetitive (typically)
    * Analog and app data is repetitive

# Chapter 4: Data Ponds

* Initially all data goes into the 'raw data pond', which is a holding area
* Data processing and conditioning lead to:
    * analog data pond
    * application data pond
    * textual data pond
* Raw data pond
    * After processing, data sets are removed from the raw data pond
* Analog data pond
    * Most analog conditioning is data reduction
* Application data pond
    * App data should already be fairly uniform and structured
    * Cross-app integration needs to happen
* Textual data pond
    * Deep analysis of textual sources requires disambiguation
    * That puts it into a uniform format and provides context
* It's possible to bypass the raw pond if your processes are good enough
* Archival data pond
    * Holds data not actively needed for analysis

# Chapter 5: Generic Structure of the Data Pond

* Common features of all ponds:
    * Pond descriptor - description of contents and data origins
        * Frequency of update / refreshment - how often data is ingested / worked on
        * Source description - lineage of the data in the pond
        * Volume of data - general description of how much data is there
        * Selection criteria - criteria used to select data for inclusion
        * Summarization criteria - description of summary algorithms
        * Organization criteria - description of data organization
        * Data relationships - description of relations between data in the pond
    * Pond target - relationship between business needs and data in the pond
        * Basic model that shapes the data in the pond
        * Can be formal or informal
        * Typical elements might include:
            * customer profile
            * sales record
            * flight manifest
            * class schedule
    * Pond data - physical data in the pond
        * Typical big data storage is as blocks with a schema stored externally
        * Schema is applied at query time
    * Pond metadata - description of the pond data
        * Dependent on data that exists outside the pond
    * Pond metaprocess - info on data transforms applied to pond data
        * Description of transforms that happen inside the data pond
        * Each pond has a different conditioning process
        * May include things like:
            * Source
            * Selection criteria
            * Frequency
            * Transform criteria
    * Pond transform criteria - documentation of how transforms should occur
        * Things like the thresholds for measurements
        * Quantization brackets, etc.

# Chapter 6: Analog Pond Data

* Often this is time series data, machine captured
* Two generic issues:
    * Total size of datasets tends to be very large
    * Much of the data tends to be utterly normal and not useful specifically
* Often has trouble with the loss of descriptor data that was associated with the measurements being taken
* You want to streamline and outline the data during transform
* Details around the analog data are very important, and may include:
    * Selection criteria for inclusion
    * Originating source
    * Frequency of data import / refresh
    * Volume of data moved into the pond
    * Date and time of the movement
* Two basic steps in moving raw data into the analog pond:
    * Capturing and moving the data
    * Transform and conditioning
* Note that the transform activity all takes place inside the pond itself
* Conditioning / transform / conversion is the particularly interesting part
* Some techniques for conditioning / data reduction:
    * Deduplication
    * Excision of unneeded data
    * Compression
    * Smoothing (outlier removal)
    * Interpolation (inferring data points not in the original set)
    * Sampling
    * Rounding (removal of unneeded precision digits)
    * Encoding (representing long data strings with shorter ones)
    * Tokenization (encoding for highly repetitive data)
    * Thresholding (excision above/below a threshold)
    * Clustering (quantization)
* Also can be useful to establish relationships between measurements
    * e.g., take a tire pressure measurement and attach manufacturer to each

# Chapter 7: Application Data Pond

* One shaping factor for app data is the 'infrastructure of the operational system.' How it was originally recorded during initial capture, and how it was settled into the originating application's storage both impact how the data works.
* Operational processing in the app determines things like:
    * Granularity of measurements
    * Organization of data
    * Data contents
    * Which business events are noteworthy
    * The timing of various events
    * How data is shaped and stored
* Descriptors for application data may include:
    * data source
    * approximate data volume
    * frequency of data ingestion
* App data often is stored as relational data within an app
* As ingested it may be transformed to different models
* App data is typically record based
* App data may or may not have structure that reflects areas of interest to the business, so transformation may include an integration step that supports those
* You need a data model for application pond data
* The data model provides high level guidance for how data should be related, via (for instance) entity-relationship, subject areas, etc.
* At a lower level the metadata contained in the data model is incredibly important
* You need to track changes to the model over time in order to ensure that data remains comparable despite (for example) metadata or schema changes
* If data makes its way into the app pond in an unintegrated state, it has to be transformed after entry
* Example case: multiple app sources encode subject marital status in different ways, and you need to standardize those before they join the pool
* You may need pointers from one application to the next, which is relatively simple--an integrated record would simply look like a merged record
* For 'intersecting' applications there is a more complex relationship. In this case you can get a record in each system for a single business event, which requires reconciliation during integration.

# Chapter 8: Textual Data Pond

* Textual data requires disambiguation, which is standardization and contextualization
* Natural language processing may involve:
    * Inline contextualization - identifying text / words by the words around them
    * Proximity analysis
    * Alternate spelling analysis
    * Homographic resolution - interpretation of words or acronyms from context
    * Acronym resolution
    * Custom variable recognition - recognizing, eg, phone numbers from format
    * Taxonomy resolution - very important part of disambiguation
    * Date standardization
* Textual data does not lend itself to a data model, so you use taxonomies and ontologies
* A taxonomy is a classification of terms, hierarchically or otherwise
* An ontology is a grouping of related taxonomies:
    * the taxonomy of 'us states' defines an internal structure for the US
    * the taxonomy of 'countries' defines which things (incl the US) are countries
    * those taxonomies together form an ontology
* Context allows textual analysis, for queries like "all occurrences of 'Joe' where context is 'army officer'"
* Text should always remain traceable to its source document

# Chapter 9: Comparing the Ponds

* Similarities across ponds--they all:
    * ingest raw data
    * transform / condition raw data
    * produce a uniform, integrated structuring of data suitable for analysis
    * support business analysis with their final output
    * ultimately feed into the archival data pond
    * have similar entry points for raw data
    * have a supporting infrastructure of documentation
* Dissimilarities:
    * raw data is very different across ponds
    * transform and conditioning processes are very different
    * type of analysis done on results is very different
* From an architectural standpoint, be very wary of moving data from pond to pond
* Even if you were to move data, you would have a hard time moving all its metadata and other infrastructure, so you risk losing context
* If you want to do analysis across ponds, you have to sync your metadata
* You may need a misc. section of the raw data pond to support business analytical processing. If you do this, maintain high standards for metadata, tranformation audit trails, etc.

# Chatper 10: Using the infrastructure

* Some hypotheticals to demonstrate that the previously described infrastructural elements are very important to analytical usefulness.

# Chapter 11: Search and Analysis

* Two basic types of search in data analysis:
    * Finding a limited amount or piece of data (last purchase by Mr. X)
    * Aggregate over a large body of data (all purchases by men)
* Barriers to search in a data lake:
    * useful data indistinguishable from noise of huge body of records
    * inability to give a particular answer with strong confidence due to not being sure about the rigor upstream of you
    * criteria for finding your data is unclear (so bad metadata, but also bad business case integration)
    * data that requires post-search transformations
    * qualifications for the data are unclear
* Pond data search should be easier because it is:
    * integrated
    * filtered
    * converted
    * organized
    * edited
* Analysis forms may include:
    * data sorting
    * summarization
    * comparative analysis
    * exception / outlier analysis
    * visualization

# Chapter 12: Business value in the data ponds

* In all ponds thare are two basic sources of analytical value:
    * Finding specific records (outage tracing, for example)
    * Finding patterns across many records

# Chapter 13: Additional topics

* Documentation of the system at every level is critical
* At the high system level:
    * how data enters the lake / pond
    * how data flows from one pond to another
    * how data flows into the archival environment
* At the detailed pond level:
    * metadata
    * metaprocess
    * transformation documentation
    * data architecture
    * criteria for selection / inclusion
    * criteria for exit

# Chapter 14: Analytical and Integration Tools

* Basically lists the kinds of tech you might need (with no real examples):
    * Data visualization
    * Search and document qualification
    * Textual disambiguation
    * Statistical analysis
    * Classical ETL

# Chapter 15: Archiving Data Ponds

* Used to hold data whose useful life has diminished, but is worth holding
* Keeps the other ponds smaller in size, more restricted to core functionality
* Some criteria for removal to archive might be:
    * aging of data
    * lowering of probability of usage
    * need to store data for legal reasons
    * need to store data for business criticality regardless of probability of use
* Data moved to the archival ponds is typically restructured
    * Metadata and metaprocess are attached directly to the raw data
    * Indexing schemes may radically change
